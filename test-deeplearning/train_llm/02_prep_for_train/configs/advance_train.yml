# model params
base_model: /home/paul/.cache/huggingface/models/unsloth--Meta-Llama-3.1-8B
model_type: LlamaForCausalLM
tokenizer_type: PreTrainedTokenizerFast

# dataset params
datasets:
  - path: jaydenccc/AI_Storyteller_Dataset
    type:
      system_prompt: ""
      field_system: system
      field_instruction: synopsis
      field_output: short_story
      format: "<|user|>\n {instruction} </s>\n<|assistant|>"
      no_input_format: "<|user|>\n {instruction} </s>\n<|assistant|>"

output_dir: ./models/Llama3_Storyteller

# model params
sequence_length: 512
bf16: auto
bf32: false

# training params
micro_batch_size: 1
num_epochs: 4
optimize: adamw_bnb_8bit
learning_rate: 0.0002

logging_steps: 1

# LoRA
adapter: lora

lora_r: 8
lora_alpha: 16
lora_dropout: 0.05

lora_target_linear: true

# Gradient Accumulation
gradient_accumulation_steps: 4

# Low Precision
load_in_8bit: true